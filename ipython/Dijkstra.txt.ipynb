{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# 9 Knotendefinitionen, A bis I\n",
      "knoten A 1\n",
      "knoten B 2 \n",
      "knoten C 3\n",
      "knoten D 4\n",
      "knoten E 5\n",
      "knoten F 6\n",
      "knoten G 7\n",
      "knoten H 8\n",
      "knoten I 9\n",
      "\n",
      "# Kantendefinitionen \n",
      "kante A B 2\n",
      "kante A F 9\n",
      "kante A G 15\n",
      "kante B C 4\n",
      "kante B G 6\n",
      "kante C D 2\n",
      "kante C I 15\n",
      "kante D E 1\n",
      "kante D I 1\n",
      "kante E F 6\n",
      "kante E H 3\n",
      "kante F H 11\n",
      "kante G H 15\n",
      "kante G I 2\n",
      "kante H I 4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from networkx import Graph, draw\n",
      "\n",
      "from scnet.grapher.reader import Reader\n",
      "from scnet.grapher.graph.drawer import GraphDrawer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from io import StringIO\n",
      "Reader(StringIO(\"\"\"\n",
      "# 9 Knotendefinitionen, A bis I\n",
      "knoten A\n",
      "knoten B \n",
      "knoten C\n",
      "knoten D\n",
      "knoten E\n",
      "knoten F\n",
      "knoten G\n",
      "knoten H\n",
      "knoten I\n",
      "\n",
      "# Kantendefinitionen \n",
      "kante A B 2\n",
      "kante A F 9\n",
      "kante A G 15\n",
      "kante B C 4\n",
      "kante B G 6\n",
      "kante C D 2\n",
      "kante C I 15\n",
      "kante D E 1\n",
      "kante D I 1\n",
      "kante E F 6\n",
      "kante E H 3\n",
      "kante F H 11\n",
      "kante G H 15\n",
      "kante G I 2\n",
      "kante H I 4\n",
      "\"\"\")).parse()\n",
      "graph = GraphDrawer().show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LexerError",
       "evalue": "cannot tokenize data: 3,8: \"knoten A\"",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mLexerError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-20-42fd66147472>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mkante\u001b[0m \u001b[0mG\u001b[0m \u001b[0mI\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mkante\u001b[0m \u001b[0mH\u001b[0m \u001b[0mI\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \"\"\")).parse()\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGraphDrawer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/cbrand/PycharmProjects/grapher/src/scnet/grapher/reader/reader.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \"\"\"\n\u001b[0;32m     65\u001b[0m         tokens = list(\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file_handle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         )\n\u001b[0;32m     68\u001b[0m         entries = list(\n",
        "\u001b[1;32m/home/cbrand/PycharmProjects/grapher/src/scnet/grapher/tokenizer/tokenizer.py\u001b[0m in \u001b[0;36mtokens\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mout\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0minput\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \"\"\"\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIGNORED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/cbrand/.virtualenvs/theoinf33/lib/python3.3/site-packages/funcparserlib/lexer.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(str)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch_specs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompiled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/cbrand/.virtualenvs/theoinf33/lib/python3.3/site-packages/funcparserlib/lexer.py\u001b[0m in \u001b[0;36mmatch_specs\u001b[1;34m(specs, str, i, position)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0merrline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mLexerError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mLexerError\u001b[0m: cannot tokenize data: 3,8: \"knoten A\""
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR: An unexpected error occurred while tokenizing input\n",
        "The following traceback may be corrupted or invalid\n",
        "The error message is: ('EOF in multi-line string', (1, 0))\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    }
   ],
   "metadata": {}
  }
 ]
}